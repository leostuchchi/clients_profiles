{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Аналитическая задача -- составить портрет клиента, склонного откликнуться на предложение о новой карте.\n",
    "#Шаг 1. Загрузка данных;\n",
    "#Шаг 2. Первичная обработка данных (при необходимости):\n",
    "#- скорректировать заголовки;\n",
    "#- скорректировать типы признаков;\n",
    "#- проверить наличие дублирующихся записей;\n",
    "#- проверить наличие аномальных значений;\n",
    "#- восстановить пропущенные значения;\n",
    "#Шаг 3. Исследовательский анализ данных\n",
    "#- в разрезе значений целевого признака (`response` -- Отклик на предложение новой карты\t) исследовать распределения признаков;\n",
    "#- исследовать возможные зависимости целевого признака от объясняющих признаков;\n",
    "#- в разрезе целевого признака составить портреты клиентов платежной системы;\n",
    "#Шаг 4. Составить и проверить гипотезы о наличие/отсутствии различий по признакам портрета клиента.\n",
    "#Шаг 5. Построить классификационные модели (дополнительное задание).\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import mannwhitneyu\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix\n",
    "import shap\n",
    "import xgboost\n",
    "\n",
    "df = pd.read_csv('/content/drive/MyDrive/Аналитик Данных/Итоговая работа/project_7/vrk_response_bank.csv', sep=';')\n",
    "#df = pd.read_csv('vrk_response_bank.csv', sep=';')\n",
    "df.shape\n",
    "\n",
    "# приводим заголовки к нижнему регистру\n",
    "df.columns = df.columns.str.lower()\n",
    "# переименуем признаки\n",
    "df = df.rename(columns={\n",
    "        'mortgage':'Ипотечный_кредит' ,\n",
    "        'life_ins':'Страхование_жизни',\n",
    "        'cre_card':'Кредитная_карта',\n",
    "        'deb_card':'Дебетовая_карта',\n",
    "        'mob_bank':'Мобильный_банк',\n",
    "        'curr_acc':'Текущий_счет',\n",
    "        'internet':'Интернет_доступ',\n",
    "        'perloan':'Индивидуальный_заем',\n",
    "        'savings':'Наличие_сбережений',\n",
    "        'atm_user':'Пользование_банкоматом_последнюю_неделю',\n",
    "        'markpl':'Пользование_маркетплейса_последний_месяц',\n",
    "        'age':'Возраст',\n",
    "        'cus_leng':'Давность_клиентской_истории'\n",
    "        #'response':'отклик'\n",
    "        })\n",
    "\n",
    "# приводим заголовки к нижнему регистру\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "# Учитывая что все признаки числовые произведем условное разделение признаков по количеству значений в признаке\n",
    "numerical = [col for col in df.columns if len(df[col].value_counts()) > 5]\n",
    "categorical = [col for col in df.columns if len(df[col].value_counts()) <= 5]\n",
    "print(numerical)\n",
    "print(categorical)\n",
    "\n",
    "# Удалим дубликаты, просмотрим изменения целевой переменной\n",
    "print('до удаления дублей', df.shape)\n",
    "print(df['response'].value_counts())\n",
    "df.drop_duplicates(inplace=True)\n",
    "print('после удаления дублей', df.shape)\n",
    "print(df['response'].value_counts())\n",
    "\n",
    "# Расчет квантилей\n",
    "def calculate_iqr_boundaries(series):\n",
    "  q25 = series.quantile(0.25)\n",
    "  q75 = series.quantile(0.75)\n",
    "  iqr = q75 - q25\n",
    "\n",
    "  boundaries = (q25 - 1.5 * iqr, q75 + 1.5 * iqr)\n",
    "  return boundaries\n",
    "\n",
    "# Заменим значения выбросов граничными значениями квантильного размаха\n",
    "numeric = df.drop(['response'], axis=1).columns\n",
    "\n",
    "for i in numerical:\n",
    "    bounds = calculate_iqr_boundaries(df[i])\n",
    "    out_l = sum(df[i] < bounds[0])\n",
    "    out_r = sum(df[i] > bounds[1])\n",
    "    # Заменяем выбросы на границы квантильного размаха\n",
    "    df.loc[df[i] < bounds[0], i] = bounds[0]\n",
    "    df.loc[df[i] > bounds[1], i] = bounds[1]\n",
    "\n",
    "    # Произведем повторный расчет выбросов\n",
    "    bounds = calculate_iqr_boundaries(df[i])\n",
    "    out_l = sum(df[i] < bounds[0])\n",
    "    out_r = sum(df[i] > bounds[1])\n",
    "    # Заменяем выбросы на границы квантильного размаха\n",
    "    df.loc[df[i] < bounds[0], i] = bounds[0]\n",
    "    df.loc[df[i] > bounds[1], i] = bounds[1]\n",
    "\n",
    "    print('-------', i, '-------')\n",
    "    print('Размах', bounds)\n",
    "    print('Количество выбросов', out_l, out_r)\n",
    "    #plt.boxplot(df[i])\n",
    "    #plt.show()\n",
    "    print('=======+++++++=======')\n",
    "\n",
    "print(df.isnull().sum())\n",
    "\n",
    "## Исследование распределения признаков в разрезе значений целевого признака (`response` -- Отклик на предложение новой карты)\n",
    "\n",
    "# Посмотрим оценки выборочных средних и медиан для каждого признака в разрезе целевого признака\n",
    "cols = numerical\n",
    "\n",
    "grouped = df.groupby('response')\n",
    "\n",
    "for col in cols:\n",
    "    mean_resp = grouped[col].mean()\n",
    "    median_resp = grouped[col].median()\n",
    "    print(f'For {col}:')\n",
    "    print('Среднее:', mean_resp)\n",
    "    print('Медиана:', median_resp)\n",
    "\n",
    "# Создаем графики распределения для каждого числового признака по значениям response\n",
    "for feature in numerical:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='response', y=feature, data=df)\n",
    "    plt.title(f'Распределение {feature} по значениям response')\n",
    "    plt.xlabel('response')\n",
    "    plt.ylabel(feature)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# гистограммы и функции плотности для каждого признака в разрезе целевого признака\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i, col in enumerate(numerical, 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    for response, data in df.groupby('response'):\n",
    "        data[col].plot(kind='kde', label=f'response {response}', alpha=0.5)\n",
    "    plt.title(col)\n",
    "    plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "column = df.drop(['response'], axis=1).columns\n",
    "# Построим графики для всех признаков\n",
    "fig, ax = plt.subplots(13, 2, figsize=(21, 45))  # Увеличил высоту\n",
    "\n",
    "for i, y in enumerate(column):\n",
    "    plt.subplot(13, 2, 2*i+1)\n",
    "    mean_0 = df[df.response == 0][y].mean()\n",
    "    median_0 = df[df.response == 0][y].median()\n",
    "\n",
    "    ax = sns.histplot(data=df[df.response == 0],\n",
    "                      x=y,\n",
    "                      color='green',\n",
    "                      label='принявшие')\n",
    "    ax = sns.histplot(data=df[df.response == 1],\n",
    "                      x=y,\n",
    "                      color='red',\n",
    "                      label='отказавшиеся')\n",
    "    ax.axvline(mean_0, color='green', label='среднее принявших')\n",
    "    ax.axvline(median_0, color='green', label='медиана принявших')\n",
    "    plt.title(y, fontsize=12)  # Добавил заголовок для графика\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(13, 2, 2*i+2)\n",
    "    mean_1 = df[df.response == 1][y].mean()\n",
    "    median_1 = df[df.response == 1][y].median()\n",
    "\n",
    "    ax = sns.kdeplot(data=df[df.response == 0],\n",
    "                      x=y,\n",
    "                      color='green',\n",
    "                      label='принявшие')\n",
    "    ax = sns.kdeplot(data=df[df.response == 1],\n",
    "                      x=y,\n",
    "                      color='red',\n",
    "                      label='отказавшиеся')\n",
    "\n",
    "    ax.axvline(mean_1, color='red', label='среднее отказавшихся')\n",
    "    ax.axvline(median_1, color='red', label='медиана отказавшихся')\n",
    "\n",
    "    plt.title(y, fontsize=12)  # Добавил заголовок для графика\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()  # Убирает перекрытие подзаголовков и графиков\n",
    "plt.show()\n",
    "\n",
    "## Исследования возможных зависимостей целевого признака от объясняющих признаков\n",
    "# Построим матрицу корреляций\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr, annot=True)\n",
    "\n",
    "# Анализ распределения числовых признаков\n",
    "sns.pairplot(df, hue='response')\n",
    "\n",
    "## Составление портретов клиентов платежной системы в разрезе целевого признака\n",
    "# Портрет клиентов с response = 0\n",
    "df_response_0 = df[df['response'] == 0]\n",
    "mean_values_response_0 = df_response_0.mean()\n",
    "# Портрет клиентов с response = 1\n",
    "df_response_1 = df[df['response'] == 1]\n",
    "mean_values_response_1 = df_response_1.mean()\n",
    "# Сравнение портретов\n",
    "print(\"Портрет клиентов с response = 0:\")\n",
    "print(mean_values_response_0)\n",
    "print(\"Портрет клиентов с response = 1:\")\n",
    "print(mean_values_response_1)\n",
    "\n",
    "# 4. Составление и проверка гипотезы о наличие/отсутствии различий по признакам портрета клиента\n",
    "# проверка нормальности распределений признаков\n",
    "numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "for feature in numeric_features:\n",
    "    stat, p = stats.shapiro(df[feature])\n",
    "    alpha = 0.05\n",
    "    if p > alpha:\n",
    "        print(f'Признак: {feature} имеет нормальное распределение (p-value={p})')\n",
    "    else:\n",
    "        print(f'Признак: {feature} не имеет нормального распределения (p-value={p})')\n",
    "\n",
    "# Создадим два датафрейма - df_response_0 и df_response_1, содержащие строки с response=0 и response=1 соответственно.\n",
    "df_response_0 = df[df['response'] == 0]\n",
    "df_response_1 = df[df['response'] == 1]\n",
    "# посчитаем средние значения признаков для каждой из групп.\n",
    "mean_values_response_0 = df_response_0.mean()\n",
    "mean_values_response_1 = df_response_1.mean()\n",
    "\n",
    "# проверим гипотезу о равенстве средних значений признаков двух групп.\n",
    "# применяем mannwhitneyu дле признаков с ненормальным распределением\n",
    "from scipy.stats import ttest_ind\n",
    "alpha = 0.05  # задаем уровень значимости\n",
    "for feature in df.columns:\n",
    "    t_stat, p_value = mannwhitneyu(df_response_0[feature], df_response_1[feature])\n",
    "    if p_value < alpha:\n",
    "        print(f\"Отвергаем нулевую гипотезу для признака {feature}: средние значения различаются\")\n",
    "    else:\n",
    "        print(f\"Не отвергаем нулевую гипотезу для признака {feature}: средние значения не различаются\")\n",
    "\n",
    "# Разделение данных на признаки и целевую переменную\n",
    "X = df.drop('response', axis=1)\n",
    "y = df['response']\n",
    "# Разделение данных на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Инициализация и обучение модели\n",
    "model = RandomForestClassifier(class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "# Прогнозирование на тестовой выборке\n",
    "y_pred = model.predict(X_test)\n",
    "# Оценка качества модели\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 score: {f1}')\n",
    "\n",
    "\n",
    "\n",
    "# Кодирование категориальных признаков трейн и тест (тест на обученном препроцессоре)\n",
    "def encode_category_feature(X, df_predict, category_feature_list):\n",
    "    encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
    "    # Применение OneHotEncoder\n",
    "    encoded_train = encoder.fit_transform(X[category_feature_list])\n",
    "    # кодируем предикт\n",
    "    encoded_predict = encoder.transform(df_predict[category_feature_list])\n",
    "    # Преобразование результата обратно в DataFrame\n",
    "    encoded_train_df = pd.DataFrame(encoded_train, columns=encoder.get_feature_names_out(category_feature_list))\n",
    "    encoded_predict_df = pd.DataFrame(encoded_predict, columns=encoder.get_feature_names_out(category_feature_list))\n",
    "    # Сброс индексов\n",
    "    X.reset_index(drop=True, inplace=True)\n",
    "    df_predict.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    encoded_train_df.reset_index(drop=True, inplace=True)\n",
    "    encoded_predict_df.reset_index(drop=True, inplace=True)\n",
    "    # Объединение с сбросом индексов\n",
    "    X_encoded = pd.concat([X, encoded_train_df], axis=1)\n",
    "    predict_encoded = pd.concat([df_predict, encoded_predict_df], axis=1)\n",
    "    # Удаление оригинального столбца\n",
    "    X_encoded = X_encoded.drop(columns=category_feature_list)\n",
    "    predict_encoded = predict_encoded.drop(columns=category_feature_list)\n",
    "\n",
    "    return X_encoded, predict_encoded\n",
    "\n",
    "\n",
    "\n",
    "def standard(X, df_predict, numeric):\n",
    "    # Применяем StandardScaler к X, numeric\n",
    "    st_scaler = StandardScaler()\n",
    "    X[numeric] = st_scaler.fit_transform(X[numeric])\n",
    "    # Применяем StandardScaler к predict\n",
    "    df_predict[numeric] = st_scaler.transform(df_predict[numeric])\n",
    "\n",
    "    return X, df_predict\n",
    "\n",
    "category = df.drop(['response', 'возраст'], axis=1).columns\n",
    "X_train_enc, X_test_enc = encode_category_feature(X_train, X_test, category)\n",
    "X_train_norm, X_test_norm = standard(X_train_enc, X_test_enc, numerical)\n",
    "# Инициализация и обучение модели\n",
    "model = RandomForestClassifier(class_weight='balanced')\n",
    "model.fit(X_train_norm, y_train)\n",
    "# Прогнозирование на тестовой выборке\n",
    "y_pred = model.predict(X_test_norm)\n",
    "# Оценка качества модели\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 score: {f1}')\n",
    "\n",
    "X_train, X_test = X_train_norm, X_test_norm\n",
    "# Логистическая регрессия\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_probs = lr_model.predict_proba(X_test)[:,1]\n",
    "roc_auc_lr = roc_auc_score(y_test, lr_probs)\n",
    "f1_lr = f1_score(y_test, lr_probs>0.5)\n",
    "confusion_lr = confusion_matrix(y_test, lr_probs>0.5)\n",
    "# Случайный лес\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_probs = rf_model.predict_proba(X_test)[:,1]\n",
    "roc_auc_rf = roc_auc_score(y_test, rf_probs)\n",
    "f1_rf = f1_score(y_test, rf_probs>0.5)\n",
    "confusion_rf = confusion_matrix(y_test, rf_probs>0.5)\n",
    "# Метод опорных векторов\n",
    "svm_model = SVC(probability=True)\n",
    "svm_model.fit(X_train, y_train)\n",
    "svm_probs = svm_model.predict_proba(X_test)[:,1]\n",
    "roc_auc_svm = roc_auc_score(y_test, svm_probs)\n",
    "f1_svm = f1_score(y_test, svm_probs>0.5)\n",
    "confusion_svm = confusion_matrix(y_test, svm_probs>0.5)\n",
    "print(\"Логистическая регрессия:\")\n",
    "print(\"ROC AUC:\", roc_auc_lr)\n",
    "print(\"F1 Score:\", f1_lr)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_lr)\n",
    "print(\"\\nCлучайный лес:\")\n",
    "print(\"ROC AUC:\", roc_auc_rf)\n",
    "print(\"F1 Score:\", f1_rf)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_rf)\n",
    "print(\"\\nМетод опорных векторов:\")\n",
    "print(\"ROC AUC:\", roc_auc_svm)\n",
    "print(\"F1 Score:\", f1_svm)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_svm)\n",
    "\n",
    "# Удаление признаков с низкой корреляцией\n",
    "def high_corr(df):\n",
    "    # сразу удалим id\n",
    "    df.drop(['id'], axis=1, inplace=True, errors='ignore')\n",
    "    min_corr = []\n",
    "    for i in df.columns:\n",
    "        j = df[i]\n",
    "        corr = df['response'].corr(j)\n",
    "        #print(i, corr)\n",
    "        if -0.01 < corr and corr < 0.01:\n",
    "            min_corr.append(i)\n",
    "\n",
    "    print(min_corr)\n",
    "    df.drop(min_corr, axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Удаление признаков с нулевой значимостью\n",
    "def important(df):\n",
    "    # Определение данных\n",
    "    X = df.drop(['response'], axis=1)\n",
    "    y = df['response']\n",
    "    # Обучение модели\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'max_depth': 3,\n",
    "        'learning_rate': 0.1\n",
    "        }\n",
    "    model = xgboost.train(params, xgboost.DMatrix(X, y), num_boost_round=10)\n",
    "    # Создание объекта Explainer и получение SHAP значений\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X)\n",
    "    # Преобразуем shap_values в DataFrame для удобной работы\n",
    "    shap_df = pd.DataFrame(shap_values, columns=X.columns)\n",
    "    # Создание переменной no_good для признаков с нулевой значимостью\n",
    "    no_good = shap_df.mean().index[shap_df.mean() == 0].tolist()\n",
    "    # Выводим список признаков с нулевой значимостью\n",
    "    print(\"Признаки с нулевой значимостью:\")\n",
    "    print(no_good)\n",
    "    df.drop(no_good, axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = high_corr(df)\n",
    "df = important(df)\n",
    "# Разделение данных на признаки и целевую переменную\n",
    "X = df.drop('response', axis=1)\n",
    "y = df['response']\n",
    "# Разделение данных на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Учитывая что все признаки числовые произведем условное разделение признаков по количеству значений в признаке\n",
    "numerical = [col for col in X_train.columns if len(X_train[col].value_counts()) > 5]\n",
    "categorical = [col for col in X_train.columns if len(X_train[col].value_counts()) <= 5]\n",
    "print(numerical)\n",
    "print(categorical)\n",
    "\n",
    "# кодируем и нормализуем\n",
    "X_train_enc, X_test_enc = encode_category_feature(X_train, X_test, categorical)\n",
    "X_train_norm, X_test_norm = standard(X_train, X_test, numerical)\n",
    "\n",
    "# Логистическая регрессия\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_probs = lr_model.predict_proba(X_test)[:,1]\n",
    "roc_auc_lr = roc_auc_score(y_test, lr_probs)\n",
    "f1_lr = f1_score(y_test, lr_probs>0.5)\n",
    "confusion_lr = confusion_matrix(y_test, lr_probs>0.5)\n",
    "\n",
    "# Случайный лес\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_probs = rf_model.predict_proba(X_test)[:,1]\n",
    "roc_auc_rf = roc_auc_score(y_test, rf_probs)\n",
    "f1_rf = f1_score(y_test, rf_probs>0.5)\n",
    "confusion_rf = confusion_matrix(y_test, rf_probs>0.5)\n",
    "\n",
    "# Метод опорных векторов\n",
    "svm_model = SVC(probability=True)\n",
    "svm_model.fit(X_train, y_train)\n",
    "svm_probs = svm_model.predict_proba(X_test)[:,1]\n",
    "roc_auc_svm = roc_auc_score(y_test, svm_probs)\n",
    "f1_svm = f1_score(y_test, svm_probs>0.5)\n",
    "confusion_svm = confusion_matrix(y_test, svm_probs>0.5)\n",
    "\n",
    "print(\"Логистическая регрессия:\")\n",
    "print(\"ROC AUC:\", roc_auc_lr)\n",
    "print(\"F1 Score:\", f1_lr)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_lr)\n",
    "print(\"\\nCлучайный лес:\")\n",
    "print(\"ROC AUC:\", roc_auc_rf)\n",
    "print(\"F1 Score:\", f1_rf)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_rf)\n",
    "print(\"\\nМетод опорных векторов:\")\n",
    "print(\"ROC AUC:\", roc_auc_svm)\n",
    "print(\"F1 Score:\", f1_svm)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_svm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ваш код охватывает множество аспектов аналитической задачи по созданию профиля клиента, откликающегося на предложение новой карты. Рассмотрим его выполнение по шагам, приведенным в задаче, и дадим рекомендации по улучшению.\n",
    "\n",
    "### Шаг 1. Загрузка данных\n",
    "\n",
    "- **Реализация**: Произведена корректная загрузка данных.\n",
    "- **Рекомендация**: Убедитесь, что путь к файлу доступен, особенно если вы планируете запускать код в разных средах.\n",
    "\n",
    "### Шаг 2. Первичная обработка данных\n",
    "\n",
    "- **Корректировка заголовков**: Заголовки приведены к нижнему регистру, что является хорошей практикой.\n",
    "- **Типы признаков**: Убедитесь, что типы признаков установлены корректно, и при необходимости выставьте их.\n",
    "- **Проверка дубликатов**: Удаление дубликатов выполнено.\n",
    "- **Проверка аномальных значений**: Идея использования IQR для выбросов хороша, однако стоит ограничить замену на границы квантиля к первой итерации. Повторное применение этой логики может привести к нежелательным результатам.\n",
    "- **Восстановление пропущенных значений**: Отсутствует. Рекомендуется добавить, например, замены медианой для числовых признаков или самым частым значением для категориальных.\n",
    "\n",
    "### Шаг 3. Исследовательский анализ данных\n",
    "\n",
    "- **Анализ распределений**: Визуализации проведены на высшем уровне. Корректная идея использования boxplot и KDE.\n",
    "- **Портрет клиентов с `response=0` и `response=1`**: Построение портретов осуществлено через средние значения. Можно дополнить детализацией, например, через средние, медианы и стандартные отклонения.\n",
    "\n",
    "### Шаг 4. Проверка гипотез\n",
    "\n",
    "- **Проверка нормальности**: Подходящая реализация, однако сам метод Шапиро-Уилка может быть чувствителен к выборкам. Посмотрите также на тесты на нормальность по другим методам (например, Kolmogorov-Smirnov).\n",
    "- **Проверка различий**: Использование теста Манна-Уитни правильно, так как данные, скорее всего, не имеют нормального распределения. Статистический анализ проведен.\n",
    "\n",
    "### Шаг 5. Построение классификационных моделей\n",
    "\n",
    "- **Создание и проверка моделей**: Модели логистической регрессии, случайного леса и метода опорных векторов построены. Результаты ROC AUC и F1 Score предоставлены.\n",
    "- **Рекомендации**:\n",
    "  - Рассмотрите пересечение методов кросс-валидации для более надежной оценки модели.\n",
    "  - Внедрите другие алгоритмы, такие как градиентный бустинг (например, LightGBM) или более сложные ансамбли.\n",
    "  - Проверьте влияние различных значений гиперпараметров, например, с использованием `GridSearchCV`.\n",
    "\n",
    "### Общие рекомендации\n",
    "\n",
    "1. **Визуализация важности признаков**: Посмотрите на важность признаков для выбранной модели (например, случайного леса), чтобы понять, какие из них вносят больше всего в предсказание.\n",
    "2. **Обработка категориальных признаков**: Убедитесь, что все методы кодирования (например, OneHotEncoding) правильно применяются к обучающей и тестовой выборкам.\n",
    "3. **Отладка после вычисления моделей**: В случае обнаружения низкой точности моделей, попробуйте выделить категориальные признаки, а затем провести дополнительные преобразования данных.\n",
    "4. **Документирование каждого шага**: Обратитесь к документации, чтобы специфицировать дополнительные комментарии к коду и объяснить каждую часть процесса.\n",
    "5. **Изучение различных метрик**: Рассмотрите использование других метрик оценки, таких как AUC-ROC и PR Curve, особенно в несбалансированных выборках.\n",
    "\n",
    "### Заключение\n",
    "\n",
    "Ваш код уже довольно хорош и охватывает большинство необходимых шагов для анализа данных и создания модели. Однако улучшения в плане проверки на пропущенные значения, отладка модели и визуализация важности признаков могут значительно повысить качество анализа и понимание ваших данных."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
